{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaneshSelvaraj717/Ganesh-Selvaraj/blob/master/Copy_of_M3_Mini_Hackathon_3_Challenge_To_Detect_Face_Masks_Using_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "another-optimum"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "\n",
        "### Kaggle Competition Notebook for Group Assignment: Challenge To Detect Face Masks Using Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "At the end of the mini-hackathon, you will be able to :\n",
        "\n",
        "* load and extract features of images \n",
        "* build the convolutional neural networks\n",
        "* use the pre-trained models of keras/Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29152de7"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This competition uses a Deep Neural Network, more specifically a Convolutional Neural Network, to differentiate between images of people, with masks, without masks and incorrectly placed masks. Manually built and pretrained networks will be used to perform this classification task.\n",
        "\n",
        "**Face-Mask-Detection-Using-CNN**\n",
        "\n",
        "* Outbreak of the Coronavirus pandemic has created various changes in the lifestyle of everyone around the world.\n",
        "* Among these changes, wearing a mask has been very vital to every individual.\n",
        "* Detection of people who are not wearing masks is a challenge due to the large populations.\n",
        "* This face mask detection project can be used in schools, hospitals, banks, airports etc as a digitalized scanning tool. \n",
        "  - The technique of detecting peopleâ€™s faces and segregating them into three classes namely the people with masks and people without masks and partial masks is done with the help of image processing and deep learning.\n",
        "* With the help of this project, a person who is monitoring the face mask status for a particular firm can be seated in a remote area and still monitor efficiently and give instructions accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "surprising-uruguay"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The data for this mini-hackathon is collected from various sources including the masked images from internet and general frontal face images considered as without mask. This dataset consists of approximately 5059 train images and approximately 1300 validation and 660 test images with 3 classes `with_mask`, `without_mask` and `partial_mask`\n",
        "\n",
        "Many people are not correctly wearing their masks due to bad practices, bad behaviors or vulnerability of individuals (e.g., children, old people). For these reasons, several mask wearing campaigns intend to sensitize people about this problem and good practices. In this sense, this work proposes three types of masked face detection dataset; namely, the Correctly Masked Face, the Incorrectly Masked Face and their combination for the global masked face detection. This dataset serves the objective of classifying faces that are: \n",
        "  \n",
        "- Without Mask/ With Mask/ Partial Mask\n",
        "  \n",
        "Note that this dataset contains some annotated (artificially generated) masks to augment the 'masked' data category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To build and implement a Convolutional Neural Network model to classify between masked/unmasked/partially masked faces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8OapRtHgLnU"
      },
      "source": [
        "### Instructions for downloading test data from kaggle are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2jS73oLnCR"
      },
      "source": [
        "### 1. Create an API key in Kaggle.\n",
        "\n",
        "To do this, go to the competition site on Kaggle at https://www.kaggle.com/t/1f1f584df43c411bace6823c74c2ea52 and open your user settings page. Click Account.\n",
        "\n",
        "![alt text](https://i.stack.imgur.com/jxGQv.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzGffHdbwX2"
      },
      "source": [
        "### 2. Next, scroll down to the API access section and click generate to download an API key (kaggle.json). \n",
        "![alt text](https://i.stack.imgur.com/Hzlhp.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtETuXx8b-OC"
      },
      "source": [
        "### 3. Upload your kaggle.json file using the following snippet in a code cell:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pfXBDxWl0Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "56aa0095-e20a-4436-c83c-52ef53d6cea9"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f9f1b544-df58-430d-bc94-20e401bc64f8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f9f1b544-df58-430d-bc94-20e401bc64f8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle (2).json to kaggle (2).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle (2).json': b'{\"username\":\"ganeshselvaraj7\",\"key\":\"57a4193f014ca69f39632cdcb70b8470\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCV_T6MMW4eX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c5333b1-8d9b-4466-bb94-7b038e2d655b"
      },
      "source": [
        "#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n",
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'kaggle (2).json'   \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbukdzJ6cE32"
      },
      "source": [
        "### 4. Install the Kaggle API using the following command\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMj1n1MJcqzN"
      },
      "source": [
        "!pip install -U -q kaggle==1.5.8"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vpy9P1nchhd"
      },
      "source": [
        "### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQbPsDOLZ0b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4bae5e-cd32-480a-83e1-35afcc223ba2"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BenAWlpI73sm"
      },
      "source": [
        "#Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2jGsCradOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c89413a-e1ea-491e-dd60-0681d0740834"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json #run this command to ensure your Kaggle API token is secure on colab"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32unPZKzdI72"
      },
      "source": [
        "### 6. Now download the Test Data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppuy5gRKHFwv"
      },
      "source": [
        "**NOTE: If you get a '404 - Not Found' error after running the cell below, it is most likely that the user (whose kaggle.json is uploaded above) has not 'accepted' the rules of the competition and therefore has 'not joined' the competition.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-ETZCE_A1j"
      },
      "source": [
        "If you encounter **401-unauthorised** download latest **kaggle.json** by repeating steps 1 & 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY40TmgfHq0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4de3d6-f5c7-43cd-d8f5-db19f243006f"
      },
      "source": [
        "#If you get a forbidden link, you have most likely not joined the competition.\n",
        "!kaggle competitions download -c challenge-to-detect-face-masks\n",
        "!unzip challenge-to-detect-face-masks.zip"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 166, in authenticate\n",
            "    self.config_file, self.config_dir))\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
            "unzip:  cannot find or open challenge-to-detect-face-masks.zip, challenge-to-detect-face-masks.zip.zip or challenge-to-detect-face-masks.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operating-latter"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOUR CODING STARTS FROM HERE"
      ],
      "metadata": {
        "id": "QeKon2vruI_c"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "812a816f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a611074-7c05-465c-96a9-4f87c0ade55b"
      },
      "source": [
        "#@title Run this cell to download the train data\n",
        "!wget -qq https://cdn.extras.talentsprint.com/DLFA/Experiment_related_data/facemask_detection.zip\n",
        "!unzip -qq facemask_detection.zip\n",
        "print(\"Data Downloaded Successfuly!!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Downloaded Successfuly!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wj3kcKEDeSXZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "### Import Required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG52PDGENRgN"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "# Importing Pytorch library\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# Matplotlib is used for ploting graphs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "### **Stage 1:** Data Loading and preprocessing of Images (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYSjwlcSGJq1"
      },
      "source": [
        "#### Analyze the shape of images and distribution of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_FC0knCfeFD"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "#data_path = \"/content/facemask_detection/train\"\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "# Comment 0: define transformation that you wish to apply on image\n",
        "#data_transforms = transforms.Compose([\n",
        "#                    transforms.CenterCrop(224),\n",
        "#                    transforms.ToTensor()])\n",
        "data_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "# Comment 1 : Load the datasets with ImageFolder\n",
        "image_trainset = datasets.ImageFolder(root= \"/content/facemask_detection/train\", transform=data_transforms)\n",
        "# Comment 2: Using the image datasets and the transforms, define the dataloaders\n",
        "#dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdQY-dzZeqWm"
      },
      "source": [
        "#### Visualize the sample images of each class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCgAcqdGewsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "9c19248f-da13-4e4f-891f-241f3754324a"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Plotting one example\n",
        "print(\"Shape of the training data (no of images, height, width) : \", image_trainset.size()) # (60000, 28, 28)\n",
        "#print(\"Shape of the testing data (no of images, height, width) : \", image_test.test_data.size())  # (10000, 28, 28)  \n",
        "print(\"\\n\")\n",
        "print(\"An Example Image\")\n",
        "plt.imshow(image_trainset.train_data[0].numpy(), cmap='gray')\n",
        "plt.title('Label : %i' % image_trainset.train_labels[0])\n",
        "plt.show()\n",
        "# helper function to un-normalize and display an image\n",
        "#def imshow(img):\n",
        "#    img = img / 2 + 0.5  # unnormalize\n",
        "#    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "        \n",
        "#dataiter = iter(dataloaders)\n",
        "#images, labels = dataiter.next()\n",
        "#images = images.numpy() # convert images to numpy for display\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "#fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "#for idx in np.arange(20):\n",
        "#    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "#    imshow(images[idx])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f514cbe6c45d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Plotting one example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of the training data (no of images, height, width) : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_trainset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (60000, 28, 28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#print(\"Shape of the testing data (no of images, height, width) : \", image_test.test_data.size())  # (10000, 28, 28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ImageFolder' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkOr1nhNT5yD"
      },
      "source": [
        "### **Stage 2:** Build and train the CNN model using Keras/Pytorch (3 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "# load data\n",
        "X_train = image_datasets\n",
        "# normalize inputs from 0-255 to 0.0-1.0\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "# Compile model\n",
        "epochs = 25\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "model.summary()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "metadata": {
        "id": "RHP1TsTFbUwM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "a0f89dc7-b3b9-4668-8bf3-d9ca2e4c58a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4b6f45d8176e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# normalize inputs from 0-255 to 0.0-1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ImageFolder' object has no attribute 'astype'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRL5GJodbRbt"
      },
      "source": [
        "### **Stage 3:** Transfer learning (3 points)\n",
        "\n",
        "Transfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem.\n",
        "\n",
        "A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task.\n",
        "\n",
        "The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the pre-trained models (VGG16, ResNet50, GoogleNet etc.)\n",
        "\n",
        "* Load the pre-trained model\n",
        "* Train and evaluate the images"
      ],
      "metadata": {
        "id": "IAKm9pm7bnwQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izxeDNkYbRbw"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Make a submission on kaggle using CNN model and Pre-trained model and then compare the model performance on the kaggle testset"
      ],
      "metadata": {
        "id": "-PIejCLa8bOC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-O0Jx99UhmI"
      },
      "source": [
        "###   **Stage 4**: Evaluate the Model and get model predictions on the Kaggle testset (2 Points)\n",
        "\n",
        "#### Expected accuracy: More than 90%\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Sw9qtXxaahOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBIA1sbLByIL"
      },
      "source": [
        "### Report Analysis\n",
        "\n",
        "- Compare the accuracies for the Pre-trained vs CNN models\n",
        "- What process was followed to tune the hyperparameters?\n",
        "- Plot the confusion matrix in terms of the misclassifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4tF71VIUwzK"
      },
      "source": [
        "### Capture the live image using the below code cell and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "I6kTLnTmrcCg"
      },
      "source": [
        "#@title Capture the photo\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n",
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNe9vDQ6q6Z6"
      },
      "source": [
        "After executing above cell and capturing the photo, load the captured photo and predict with model.\n",
        "\n",
        "**Note:** \n",
        "* Convert the image to numpy array and resize to the shape which model accept. \n",
        "* Extend the dimension (to 4-D shape) of an image, as the model is trained on a batch of inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD5gr9YOAX83"
      },
      "source": [
        "import PIL\n",
        "from matplotlib import pyplot as plt\n",
        "features = PIL.Image.open(\"photo.jpg\")\n",
        "plt.imshow(features);\n",
        "# YOUR CODE HERE to predict the image"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}